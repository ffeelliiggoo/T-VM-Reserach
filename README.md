# Closing the Cloud Detection Gap: Maturing T&VM with CSA CCM and MITRE ATT&CK

Felipe E. Gonzalez
Cloud Information Security Advisor | Threat Detection & Vulnerability Management | Cloud Control Adherence & Maturity | CSPM & CNAPP


April 6, 2025
Abstract
Cloud vulnerabilities often pose hidden risks, not because they evade detection, but because they remain unaddressed. This article presents two scenarios where detection failed to drive remediation: one involving an untagged asset breaching SLA, and another where a critical issue was buried in alert noise. Using the Cloud Security Alliance‚Äôs Cloud Controls Matrix (CSA CCM) and the MITRE ATT&CK framework, the analysis maps specific control gaps and highlights how adversary context (e.g., techniques like T1133 and T1486) can drive urgency and accountability. 
Closing the Cloud Detection Gap: Maturing T&VM with CSA CCM and MITRE ATT&CK


The goal is to reinforce why post-detection maturity is essential in preventing otherwise avoidable security incidents.

Introduction
In cloud security, identifying a vulnerability is just the beginning, what follows determines the organization‚Äôs actual risk posture. A detection without timely response is a missed opportunity to prevent an imminent compromise. While not every issue leads to a breach, assuming a discovered weakness can be deprioritized is a dangerous gamble. As the Cloud Security Alliance (CSA) notes, a mature Threat and Vulnerability Management (TVM) program must not only surface issues but ensure they are assigned, prioritized, and remediated effectively. 

Even a single unpatched flaw can provide an entry point for an attacker.
This article examines two illustrative scenarios where detection did not translate into timely action. Each exposes breakdowns in governance and workflows that allowed critical vulnerabilities to linger past SLA. These are then mapped to control failures within the CSA Cloud Controls Matrix (CCM) and enriched with MITRE ATT&CK adversary context to show how post-detection maturity can and should bridge the gap between finding a problem and fixing it.


Drafted in collaboration with OpenAI‚Äôs GPT-4o.


‚òÅÔ∏è When No One Owns the Risk (Lost Untagged Workload)
A routine cloud scan flags a critical vulnerability on a server: an outdated library enabling remote code execution (RCE). A remediation ticket is generated with a 48-hour SLA as per policy. However, the affected server is untagged and missing from the CMDB, so no owner is auto-assigned. For days, the ticket sits idle with no notifications or accountability. 

Only during a weekly audit does a security analyst notice the overdue ticket. After scrambling to investigate, engineers discover it‚Äôs a forgotten test instance exposed in a production environment. The server is quickly terminated, but the delay highlights a major breakdown: detection succeeded, but lack of ownership and process controls led to inaction.

üïí Vulnerability Response Timeline ‚Äì Breakdown
Hour 0: üî¥ Critical vulnerability detected. An automated scanner finds a severe RCE flaw on a cloud workload and opens a ticket with a 48-hour remediation SLA. Ideally, this should also trigger an owner assignment, but the asset‚Äôs metadata is incomplete.
Hours 6‚Äì48: ‚ö†Ô∏è SLA breach occurs. The 48-hour window passes with the ticket unassigned. Because the server had no tags and wasn‚Äôt in the inventory, normal routing and notification workflows fail. No automated escalation happens for this orphaned issue, so the missed SLA initially goes unnoticed. The vulnerability remains open and risk exposure continues.
Day 7: ‚è≥ Issue flagged during audit. In a weekly SLA review, a security analyst spots the unresolved critical finding. This prompts a manual escalation to cloud operations. It‚Äôs now clear the issue sat a week with no action. The vulnerability has persisted in production 7 days post-detection (SLA violated by 5 days).
Day 10: üîç Asset traced to out-of-compliance instance. Cloud engineers identify the server as a forgotten test instance running in a production subnet. It was deployed without proper approvals or tagging ‚Äî a direct result of missing governance (no service control policies or change management checks). The instance was out-of-compliance from the start. The team also realizes improper IAM setup prevented the security team from remediating the server directly; they had to locate someone with the right cloud privileges.
Day 14: üõ†Ô∏è Remediation finally begins (SLA extended). Two weeks after detection, responsible engineers begin remediation. A formal SLA extension is requested since the 48-hour SLA is now a 336-hour (~14 day) violation. The team decides whether to patch the vulnerable library or simply terminate the rogue instance. They generate a Software Bill of Materials (SBOM) for the server to inventory all software components. The SBOM confirms the exact vulnerable library version and helps check if other systems use the same component. This information is invaluable; it guides the fix and ensures no similar forgotten instances or images harbor the same flaw.

üîé Gaps Discovered:
Analyzing this incident through the Cloud Controls Matrix (CCM) reveals at least four critical control failures, each contributing to extended exposure. With adherence well below 25%, the following gaps allowed the vulnerability to persist:

TVM-01 ‚Äì Vulnerability Management Policy & Procedures: A lack of enforcement around identifying and assigning ownership meant the untagged asset was never tracked or remediated. An effective TVM-01 policy would have ensured immediate ownership and accountability.
TVM-09 ‚Äì Vulnerability Tracking & Stakeholder Notification: No stakeholder was alerted because no owner was defined at detection. This violates TVM-09, which requires clear tracking and notification processes from discovery through resolution.
DCS-06 ‚Äì Asset Inventory and Tracking: The untagged server bypassed asset governance entirely. Enforcing DCS-06 through tagging policies and inventory integration would have flagged the asset immediately or prevented its deployment.
AIS-06 ‚Äì Secure and Standardized Deployment: Without golden images or deployment gates, teams spun up outdated, non-compliant instances. Proper adherence to AIS-06 would have ensured only approved, up-to-date resources entered production.

üß©Recommendation: 
To avoid a recurrence of this scenario, the organization should focus on four key improvements:

Enforce Tagging and Ownership: Require all cloud assets to be tagged with ownership metadata and enforce this through automated policies. Assign fallback owners for orphaned resources to ensure every asset is covered.
Automate SLA Escalation: Implement tools to monitor SLA compliance. If a critical issue remains unassigned or unresolved past the SLA, trigger automatic alerts to relevant stakeholders for immediate escalation.
Deploy Preemptive Guardrails: Use policy-as-code and CI/CD integration to block non-compliant resources (e.g., untagged or outdated images) from entering production. Preventive controls reduce the volume of issues requiring post-deployment remediation.
Strengthen IAM Access: Ensure security teams have visibility and access across environments. Apply least-privilege principles and restrict unauthorized deployments by enforcing oversight on provisioning roles.

These changes establish consistent ownership, streamline escalation, and embed security earlier in the lifecycle‚Äîhelping to close the gap between detection and remediation in Scenario 1.


Drafted in collaboration with OpenAI‚Äôs GPT-4o.
üßæ When Too Many Alerts Hide the Real Threats?
A cloud vulnerability scan generates thousands of findings, mostly low-risk. Among them is one critical vulnerability on a widely used web server ‚Äì but it nearly gets lost in the flood of alerts. The environment again lacks strong governance guardrails: there‚Äôs no enforcement of golden base images or baseline configurations, so many systems have minor known flaws that inflate scan results. 

No pre-deployment security gates exist to catch these issues earlier, meaning a large number of low-priority problems make it to production unchecked. The result is an overwhelming volume of alerts with little context to distinguish real danger. The critical alert fails to get the urgent attention it warrants, not due to ignorance of its severity, but because it‚Äôs hidden in the noise and the response process doesn‚Äôt elevate it in time.

üïí Vulnerability Response Timeline ‚Äì Breakdown
Day 0: üî¥ Critical vulnerability detected within scan results A routine scan reports over 3,000 findings. Among them is a critical vulnerability affecting a widely used web application server. A ticket is created with a 48-hour SLA but enters the same queue as hundreds of lower-severity items. Without additional context or prioritization, it fails to draw immediate attention during triage.
Days 1‚Äì4: üì• Alert overlooked amidst backlog Security teams begin working through the findings, but focus is diluted by numerous non-critical issues. The lack of risk-based triage or automation means the critical item remains buried. The 48-hour SLA expires unnoticed, as no escalation triggers are in place to catch overdue high-severity items.
Day 5: ‚è≥ Critical issue identified during review During a status check, an analyst highlights unresolved critical findings. The vulnerability is rediscovered‚Äînow one day past SLA. Manual processes delayed detection of the oversight. A properly configured dashboard would have surfaced the overdue item earlier.
Day 6: üì¢ Escalation to application owner The issue is escalated to the responsible team. A high-priority meeting is called between Security, IT, and application stakeholders to assess the risk. At this point, the vulnerability has remained unpatched for nearly a week, raising concern across leadership.
Days 7‚Äì14: üõ†Ô∏è Assessment and SLA extension The application team confirms the issue is valid but requires complex remediation, such as a major library upgrade. An extension is requested to accommodate testing and safe deployment. The team uses the application‚Äôs SBOM to trace the vulnerable library and identifies two other applications sharing the same component, prompting broader remediation efforts.
Days 15‚Äì30: üöß Remediation and rollout Remediation progresses over the following weeks, involving patching, validation, and coordinated release. The security team monitors updates closely. The delay, caused by poor triage and lack of prioritization, results in a month-long exposure to a known critical risk‚Äîfar exceeding the original 2-day SLA.

üîé Gaps Discovered:
This scenario reveals poor adherence to several CSA CCM controls within the Threat & Vulnerability Management domain, with overall implementation below 50%. Key deficiencies included weak prioritization, delayed response, and lack of visibility into remediation progress:

TVM-03 ‚Äì Emergency Vulnerability Response: There was no mechanism to fast-track critical issues, resulting in the alert being treated like routine findings. A proper emergency process would have triggered immediate action on Day 0.
TVM-08 ‚Äì Risk-Based Prioritization: All findings were placed in a single queue without context or scoring. Risk-aware tagging would have elevated the critical item for faster review.
TVM-09 ‚Äì Stakeholder Notification: Although the asset had an owner, there was no timely alert or escalation when the SLA was missed. A defined notification process would have prompted intervention by Day 2, not Day 6.
TVM-10 ‚Äì SLA Monitoring and Metrics: No dashboards or reports tracked overdue criticals. The SLA overrun went unnoticed until it was manually discovered. Real-time monitoring would have surfaced the delay much earlier.

Together, these control failures allowed a known, high-risk vulnerability to remain unresolved for nearly a month. Strengthening these areas would have ensured faster detection, clearer accountability, and timely remediation.

üß© Recommendation:
To address the gaps seen in Scenario 2, the organization should improve both its vulnerability management process and its use of contextual information:

Prioritize by Risk, Not Volume: Implement automated triage that tags and routes vulnerabilities based on risk‚Äînot severity alone. Use contextual scoring (e.g., exploitability, asset criticality, exposure) to surface high-impact findings. Critical issues should be routed to a priority queue with immediate visibility across teams.
Escalate SLA Breaches Automatically: Configure alerting rules for any critical vulnerability that exceeds its SLA. After 48 hours without progress, trigger notifications to both asset owners and senior security leaders. Supplement with daily SLA dashboards to ensure accountability doesn‚Äôt rely on manual tracking.
Use Threat Context to Drive Urgency: Link critical findings to known MITRE ATT&CK techniques and exploit data during triage. For example, identifying a vulnerability that maps to T1486 (ransomware behavior) immediately elevates urgency. Embedding threat context enables faster, better-informed decisions.
Cut Noise Upstream: Shift left by improving development security hygiene. Enforce patching policies pre-deployment, mandate hardened golden images, and scan infrastructure as code (IaC). Reducing low-value alerts at the source prevents real threats from being drowned in noise later

These improvements ensure that critical vulnerabilities are identified and addressed promptly ideally within hours. Risk-based filtering, automated escalation, and enriched threat context help prevent high-severity issues from being overlooked. In a mature process, such gaps would be caught early, not days later.


üëì Seeing Risk Through the MITRE ATT&CK Lens
The CSA CCM analysis above identifies process and control gaps, but MITRE‚Äôs ATT&CK framework complements this by mapping vulnerabilities to real-world adversary behaviors. While the CCM highlights what is broken in our defenses, ATT&CK shows how those weaknesses could be exploited, adding crucial threat context that can inform prioritization and remediation.

For example, consider the specific vulnerabilities in our scenarios:

In Scenario 1 (the untagged forgotten workload), suppose the critical vulnerability turned out to be an exposed remote administration interface with default credentials. That‚Äôs exactly the kind of weakness attackers exploit for initial access. In ATT&CK terms, it maps to T1133: External Remote Services ‚Äì a technique where adversaries leverage exposed services like RDP or VPN to gain access. Recognizing this mapping would tell the team that this isn‚Äôt just a misconfiguration; it‚Äôs a potential foothold for an attacker, warranting immediate action.
In Scenario 2 (the signal lost in noise), say the nearly missed critical flaw would allow an attacker to encrypt data or deploy ransomware if exploited. This maps to T1486: Data Encrypted for Impact ‚Äì the ATT&CK technique for encrypting data to cause disruption (essentially a ransomware attack). In other words, that vulnerability could enable a high-impact attack. If the team had this insight on Day 0, they would have realized the finding was an imminent threat.

‚ö° From Alerts to Action: Aligning Response with ATT&CK
1) If an attacker targets this, what could they do? 2) Which ATT&CK techniques would it enable?‚Äù 
Integrating MITRE ATT&CK into vulnerability management adds threat context that improves prioritization and communication across teams.

üéØ Threat-Relevant Prioritization Not all critical vulnerabilities carry equal risk. A vulnerability tied to a common attack technique‚Äîsuch as credential dumping or privilege escalation‚Äîwarrants faster response than one less likely to be exploited. By mapping findings to ATT&CK tactics and known adversary behaviors, teams can focus on the threats most likely to be used in real-world attacks. In both scenarios, identifying links to techniques like Initial Access or Impact would have helped elevate priority and reduce response time.

ü§ù Shared Language Across Teams MITRE ATT&CK offers a standardized way to describe threats. For example, referencing a vulnerability as ‚ÄúT1133: External Remote Services‚Äù immediately conveys risk to SOC analysts, incident responders, and developers. This common framework improves communication and shared understanding across technical and non-technical teams.

üìã Validating Controls and Coverage Mapping vulnerabilities to ATT&CK also tests whether controls are in place to detect or prevent related techniques. Scenario 1‚Äôs vulnerability aligned with an external access vector‚Äîprompting the question, are external login attempts or open ports being monitored? Scenario 2‚Äôs ransomware-related issue should trigger checks for encryption behavior or privilege misuse. While CSA CCM ensures foundational controls are present, ATT&CK confirms whether they align with actual attacker behavior.

Many modern tools include ATT&CK integration, allowing teams to quickly associate CVEs with tactics like Privilege Escalation or Defense Evasion. Dashboards and filters can help focus remediation on the most dangerous findings.

In short, mapping high-risk issues to ATT&CK brings an adversary lens into play‚Äîhelping prioritize, communicate, and validate defenses. It ensures that detection is followed by targeted action, not just generic remediation.



üèÅ Final Takeaway
Detection without action is deferred risk. The two scenarios explored illustrate that even with capable tools, the absence of ownership, prioritization, and structured response allows critical vulnerabilities to persist. Each gap revealed is a path to maturity, not just a lesson, but a clear directive.

1. Build Guardrails Upstream: Prevention starts before deployment. Enforcing policies like mandatory tagging, approved hardened images, and CI/CD security gates ensures vulnerabilities are less likely to enter production. Governance is not optional‚Äîit‚Äôs the baseline for resilient operations.

2. Operationalize TVM Workflows: Effective vulnerability management requires automated assignment, SLA tracking, and escalation‚Äînot just tickets. Detection must be tied to accountability from Day 0. CCM-aligned audits help validate that nothing slips through the cracks.

3. Make Responsibility Explicit: Security is everyone‚Äôs job. Clear ownership, supported by collaborative processes and transparent escalation, prevents the ‚ÄúI thought someone else was handling it‚Äù dilemma. Culture and process must reinforce each other.

4. Prioritize with Context: Tools like MITRE ATT&CK and SBOMs turn alerts into insight. Mapping findings to known attacker behaviors clarifies urgency, while SBOMs identify exposure across systems. These resources are no longer nice-to-haves‚Äîthey‚Äôre essential to informed defense.

Cloud security maturity isn‚Äôt a milestone; it‚Äôs a continuous cycle ‚ùó
Each miss, delay, or false assumption presents an opportunity to reinforce the system. With strong governance, clear ownership, and threat-informed decision-making, critical vulnerabilities can be addressed swiftly and confidently.

https://www.linkedin.com/pulse/building-resilient-enterprise-research-based-approach-gonzalez-megne/?trackingId=dhl1j5MmSoOce2fTbryn4Q%3D%3D
